{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Link found in 16's url and External Link Found In 2987's url\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "resp = urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Swami_Vivekananda\")\n",
    "soup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "internalCount = 0\n",
    "externalCount = 0\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if re.search(\"en.wikipedia.org\",link['href']):\n",
    "        internalCount+=1;\n",
    "    else:\n",
    "        externalCount+=1\n",
    "print(f\"Internal Link found in {internalCount}'s url and External Link Found In {externalCount}'s url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import urllib.request\n",
    "# import re\n",
    "# parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "# resp = urllib.request.urlopen(\"https://www.amazon.in/\")\n",
    "# with open('test.html') as hf:\n",
    "#     soup = BeautifulSoup(hf, 'html.parser')\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "resp = urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Rabindranath_Tagore\")\n",
    "soup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "# soup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "internalCount = SET()\n",
    "# externalCount = 0\n",
    "for a_tag in soup.findAll(\"a\"):\n",
    "    href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "        # join the URL if it's relative (not absolute link)\n",
    "#         href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        # remove URL GET parameters, URL fragments, etc.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "https = 0\n",
    "http = 0\n",
    "# print(len(soup.find_all('a')))\n",
    "for link in soup.find_all('a'):\n",
    "        if re.search(\"https\",link['href']):\n",
    "            https+=1\n",
    "        else:\n",
    "            http+=1\n",
    "    \n",
    "print(f\"{https}\\n{http}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Total Internal links: 1\n",
      "[+] Total External links: 319\n",
      "[+] Total URLs: 320\n",
      "[+] Total crawled URLs: https://en.wikipedia.org/wiki/Rabindranath_Tagore\n"
     ]
    }
   ],
   "source": [
    "from requests_html import HTMLSession\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import colorama\n",
    "\n",
    "# init the colorama module\n",
    "colorama.init()\n",
    "\n",
    "GREEN = colorama.Fore.GREEN\n",
    "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
    "RESET = colorama.Fore.RESET\n",
    "YELLOW = colorama.Fore.YELLOW\n",
    "\n",
    "# initialize the set of links (unique links)\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "\n",
    "total_urls_visited = 0\n",
    "\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_all_website_links(url):\n",
    "    \"\"\"\n",
    "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
    "    \"\"\"\n",
    "    # all URLs of `url`\n",
    "    urls = set()\n",
    "    # domain name of the URL without the protocol\n",
    "    domain_name = urlparse(url).netloc\n",
    "    # initialize an HTTP session\n",
    "    session = HTMLSession()\n",
    "    # make HTTP request & retrieve response\n",
    "    response = session.get(url)\n",
    "    # execute Javascript\n",
    "    try:\n",
    "        response.html.render()\n",
    "    except:\n",
    "        pass\n",
    "    soup = BeautifulSoup(response.html.html, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "        # join the URL if it's relative (not absolute link)\n",
    "#         href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        # remove URL GET parameters, URL fragments, etc.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            # not a valid URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # already in the set\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # external link\n",
    "            if href not in external_urls:\n",
    "#                 print(f\"{GRAY}[!] External link: {href}{RESET}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "#         print(f\"{GREEN}[*] Internal link: {href}{RESET}\")\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "#     parser = argparse.ArgumentParser(description=\"Link Extractor Tool with Python\")\n",
    "#     parser.add_argument(\"url\", help=\"The URL to extract links from.\")\n",
    "#     parser.add_argument(\"-m\", \"--max-urls\", help=\"Number of max URLs to crawl, default is 30.\", default=30, type=int)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     url = args.url\n",
    "#     max_urls = args.max_urls\n",
    "    url = \"https://en.wikipedia.org/wiki/Rabindranath_Tagore\"\n",
    "#     max_urls = url\n",
    "#     crawl(url,30)\n",
    "    get_all_website_links(url)\n",
    "\n",
    "    print(\"[+] Total Internal links:\", len(internal_urls))\n",
    "    print(\"[+] Total External links:\", len(external_urls))\n",
    "    print(\"[+] Total URLs:\", len(external_urls) + len(internal_urls))\n",
    "    print(\"[+] Total crawled URLs:\", max_urls)\n",
    "\n",
    "#     domain_name = urlparse(url).netloc\n",
    "\n",
    "    # save the internal links to a file\n",
    "#     with open(f\"{domain_name}_internal_links.txt\", \"w\") as f:\n",
    "#         for internal_link in internal_urls:\n",
    "#             print(internal_link.strip(), file=f)\n",
    "\n",
    "    # save the external links to a file\n",
    "#     with open(f\"{domain_name}_external_links.txt\", \"w\") as f:\n",
    "#         for external_link in external_urls:\n",
    "#             print(external_link.strip(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
